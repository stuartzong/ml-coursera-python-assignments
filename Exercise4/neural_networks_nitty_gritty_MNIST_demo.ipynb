{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Here I am writing what I have learned in great nitty gritty details about neural networks deep learning. The contents are mostly based on Andrew Ng's machine learning course on Coursera (https://www.coursera.org/learn/machine-learning). I strongly believe that you need to implement algorithms such as feedforward, back propagation, gradient descend, and cost function yourself in order to truly understand the inner working of a neural network. I hope my understanding will be enhanced and others may be able to pick up one insight or two.\n",
    "\n",
    "### Objective\n",
    "Here, I work with the MNIST data set. The objective is to build a simply neural network to recognize those hand-written digits. My focus here is to deeply understand how the neural network learn through implementing the cost function, feed forwrad, back propagation process.\n",
    "\n",
    "### neural network model representation\n",
    "\n",
    "The neural network is shown in the following figure. Briefly, This simple neural network has 3 layers: an input layer(l1), a hidden layer(l2) and an output layer(l3). Input layer l1 has 401 units (400 image pixels + 1 bias). Hidden layer l2 has 26 units (25 units derived from l1 + 1 bias). The output layer l3 has 10 units, with each representing the probability of being hand-written number 0 to 9.\n",
    "\n",
    "![](Figures/neural_network.png)\n",
    "\n",
    "### forward pass\n",
    "The forward pass happens when we feed the input signal to the neural network and let it predict the 10 probabilities of the image being each of the 10 numbers. The input signal is values of 400 gray scale intensity of an hand-written image in this case. \n",
    "\n",
    "What really happens in forward pass is just some simple mathatical operations, namely matrix multiplication and activation. Let's get to the math behind.\n",
    "\n",
    "#### forward pass (weight x signal + bias)\n",
    "This a two-step process. First, we compute the sum of every value of the units in current layer times their corresponding weights. Then we feed the sum into a activation function. In this case, we choose sigmoid function, which squashes the numbers into a value between 0 and 1. \n",
    "\n",
    "example to illustrate forward pass:\n",
    "\n",
    "\n",
    "Numpy provide matmul or @ operation for matrix multiplication. This is different from multiple or *. \n",
    "\n",
    "#### back propagation\n",
    "1. compute delta2, which is the error between prediction and label. For one training image, it is just prediction - label for each output unit, which results in a vector with shape (10,). \n",
    "\n",
    "In this case, for 1 image, use parenthesis instead of square bracket\n",
    "\n",
    "prediction is: (9.95734012e-01 1.12661530e-04 1.74127856e-03 2.52696959e-03\n",
    " 1.84032321e-05 9.36263860e-03 3.99270267e-03 5.51517524e-03\n",
    " 4.01468105e-04 6.48072305e-03) \n",
    " \n",
    " the label is: (1. 0. 0. 0. 0. 0. 0. 0. 0. 0. )\n",
    " \n",
    "delta2 is: (-4.26598801e-03  1.12661530e-04  1.74127856e-03  2.52696959e-03\n",
    "  1.84032321e-05  9.36263860e-03  3.99270267e-03  5.51517524e-03\n",
    "  4.01468105e-04  6.48072305e-03)\n",
    "\n",
    "2. now we need to back propagate the error from the output layer(l3) to the hidden layer (l2)\n",
    "\n",
    "the error of layer2 is the product of 2 components. \n",
    "\n",
    "one is the error from layer3 times the weights(l2 > l3 weights) \n",
    "\n",
    "second is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils\n",
    "\n",
    "# define the submission/grader object for this exercise\n",
    "grader = utils.Grader()\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission and Grading\n",
    "\n",
    "\n",
    "After completing each part of the assignment, be sure to submit your solutions to the grader. The following is a breakdown of how each part of this exercise is scored.\n",
    "\n",
    "\n",
    "| Section | Part                                             | Submission function | Points \n",
    "| :-      |:-                                                | :-                  | :-:    \n",
    "| 1       | [Feedforward and Cost Function](#section1)                    | [`nnCostFunction`](#nnCostFunction)   | 30     \n",
    "| 2       | [Regularized Cost Function](#section2)                        | [`nnCostFunction`](#nnCostFunction)   | 15     \n",
    "| 3       | [Sigmoid Gradient](#section3)                                 | [`sigmoidGradient`](#sigmoidGradient) | 5      \n",
    "| 4       | [Neural Net Gradient Function (Backpropagation)](#section4)   | [`nnCostFunction`](#nnCostFunction)   | 40     \n",
    "| 5       | [Regularized Gradient](#section5)                             | [`nnCostFunction`](#nnCostFunction)   |10     \n",
    "|         | Total Points                                     |    | 100    \n",
    "\n",
    "\n",
    "You are allowed to submit your solutions multiple times, and we will take only the highest score into consideration.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "At the end of each section in this notebook, we have a cell which contains code for submitting the solutions thus far to the grader. Execute the cell to see your score up to the current section. For all your work to be submitted properly, you must execute those cells at least once.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "In the previous exercise, you implemented feedforward propagation for neural networks and used it to predict handwritten digits with the weights we provided. In this exercise, you will implement the backpropagation algorithm to learn the parameters for the neural network.\n",
    "\n",
    "We start the exercise by first loading the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  training data stored in arrays X, y\n",
    "data = loadmat(os.path.join('Data', 'ex4data1.mat'))\n",
    "X, y = data['X'], data['y'].ravel()\n",
    "\n",
    "# set the zero digit to 0, rather than its mapped 10 in this dataset\n",
    "# This is an artifact due to the fact that this dataset was used in \n",
    "# MATLAB where there is no index 0\n",
    "y[y == 10] = 0\n",
    "\n",
    "# Number of training examples\n",
    "m = y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]), (5000, 400))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X.shape\n",
    "y.shape\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Visualizing the data\n",
    "\n",
    "You will begin by visualizing a subset of the training set, using the function `displayData`, which is the same function we used in Exercise 3. It is provided in the `utils.py` file for this assignment as well. The dataset is also the same one you used in the previous exercise.\n",
    "\n",
    "There are 5000 training examples in `ex4data1.mat`, where each training example is a 20 pixel by 20 pixel grayscale image of the digit. Each pixel is represented by a floating point number indicating the grayscale intensity at that location. The 20 by 20 grid of pixels is “unrolled” into a 400-dimensional vector. Each\n",
    "of these training examples becomes a single row in our data matrix $X$. This gives us a 5000 by 400 matrix $X$ where every row is a training example for a handwritten digit image.\n",
    "\n",
    "$$ X = \\begin{bmatrix} - \\left(x^{(1)} \\right)^T - \\\\\n",
    "- \\left(x^{(2)} \\right)^T - \\\\\n",
    "\\vdots \\\\\n",
    "- \\left(x^{(m)} \\right)^T - \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The second part of the training set is a 5000-dimensional vector `y` that contains labels for the training set. \n",
    "The following cell randomly selects 100 images from the dataset and plots them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1346 2092 3748 4314 1680 2146   38  375  829 4277 3354 4111   69 4250\n",
      " 2161 1927 1682 3199 3072 4176 1836 3018 4187 3907 1099 2880 2672  577\n",
      " 1558 1498 3396 1846 4460 4254 1227 3202 4959 1112 2443 4310 1479  681\n",
      " 3007 3601 4077 4436 1252 3896  185 2228  270  120 3038 1031 3256 1036\n",
      " 1405 2050  514 2895  608 4165 4503 1029 2537 4384 1578 3726 3050 3570\n",
      " 1542 1628 2057 4971 4407 3342 3719 1066 4178 4977 1126  844 3181 2761\n",
      " 4309 4255 4582 2116 1681 1893 2630 2656  144 3146 3529 3808  236 3291\n",
      " 1108 2264]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 100 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly select 100 data points to display\n",
    "rand_indices = np.random.choice(m, 100, replace=False)\n",
    "print(rand_indices)\n",
    "sel = X[rand_indices, :]\n",
    "\n",
    "utils.displayData(sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model representation\n",
    "\n",
    "Our neural network is shown in the following figure.\n",
    "\n",
    "![](Figures/neural_network.png)\n",
    "\n",
    "It has 3 layers - an input layer, a hidden layer and an output layer. Recall that our inputs are pixel values\n",
    "of digit images. Since the images are of size $20 \\times 20$, this gives us 400 input layer units (not counting the extra bias unit which always outputs +1). The training data was loaded into the variables `X` and `y` above.\n",
    "\n",
    "You have been provided with a set of network parameters ($\\Theta^{(1)}, \\Theta^{(2)}$) already trained by us. These are stored in `ex4weights.mat` and will be loaded in the next cell of this notebook into `Theta1` and `Theta2`. The parameters have dimensions that are sized for a neural network with 25 units in the second layer and 10 output units (corresponding to the 10 digit classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10, 26)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10285,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the parameters you will use for this exercise\n",
    "input_layer_size  = 400  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25   # 25 hidden units\n",
    "num_labels = 10          # 10 labels, from 0 to 9\n",
    "\n",
    "# Load the weights into variables Theta1 and Theta2\n",
    "weights = loadmat(os.path.join('Data', 'ex4weights.mat'))\n",
    "\n",
    "# The dimensions of thetas is always (next_hidden_layer_size, previous_layer_size+1)\n",
    "# Theta1 has size 25 x 401\n",
    "# Theta2 has size 10 x 26\n",
    "Theta1, Theta2 = weights['Theta1'], weights['Theta2']\n",
    "\n",
    "# swap first and last columns of Theta2, due to legacy from MATLAB indexing, \n",
    "# since the weight file ex3weights.mat was saved based on MATLAB indexing\n",
    "Theta2 = np.roll(Theta2, 1, axis=0)\n",
    "Theta1.shape\n",
    "Theta2.shape\n",
    "\n",
    "# Unroll parameters \n",
    "nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])\n",
    "nn_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  0,  1],\n",
       "       [ 6,  7,  4,  5],\n",
       "       [10, 11,  8,  9]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8,  9, 10, 11],\n",
       "       [ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[11,  0,  1,  2],\n",
       "       [ 3,  4,  5,  6],\n",
       "       [ 7,  8,  9, 10]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.roll: this shift rows or columns and move the extra rows/columns to the beginning\n",
    "a = np.arange(12)\n",
    "arr = a.reshape(3,4)\n",
    "arr\n",
    "np.roll(arr, 2,  axis=1)\n",
    "np.roll(arr, 1, axis=0)\n",
    "# element_wise roll\n",
    "np.roll(arr, 1)\n",
    "arr.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "### 1.3 Feedforward and cost function\n",
    "\n",
    "Now you will implement the cost function and gradient for the neural network. First, complete the code for the function `nnCostFunction` in the next cell to return the cost.\n",
    "\n",
    "Recall that the cost function for the neural network (without regularization) is:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right]$$\n",
    "\n",
    "where $h_\\theta \\left( x^{(i)} \\right)$ is computed as shown in the neural network figure above, and K = 10 is the total number of possible labels. Note that $h_\\theta(x^{(i)})_k = a_k^{(3)}$ is the activation (output\n",
    "value) of the $k^{th}$ output unit. Also, recall that whereas the original labels (in the variable y) were 0, 1, ..., 9, for the purpose of training a neural network, we need to encode the labels as vectors containing only values 0 or 1, so that\n",
    "\n",
    "$$ y = \n",
    "\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\\\vdots \\\\ 0 \\end{bmatrix}, \\quad\n",
    "\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}, \\quad \\cdots  \\quad \\text{or} \\qquad\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "For example, if $x^{(i)}$ is an image of the digit 5, then the corresponding $y^{(i)}$ (that you should use with the cost function) should be a 10-dimensional vector with $y_5 = 1$, and the other elements equal to 0.\n",
    "\n",
    "You should implement the feedforward computation that computes $h_\\theta(x^{(i)})$ for every example $i$ and sum the cost over all examples. **Your code should also work for a dataset of any size, with any number of labels** (you can assume that there are always at least $K \\ge 3$ labels).\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "**Implementation Note:** The matrix $X$ contains the examples in rows (i.e., X[i,:] is the i-th training example $x^{(i)}$, expressed as a $n \\times 1$ vector.) When you complete the code in `nnCostFunction`, you will need to add the column of 1’s to the X matrix. The parameters for each unit in the neural network is represented in Theta1 and Theta2 as one row. Specifically, the first row of Theta1 corresponds to the first hidden unit in the second layer. You can use a for-loop over the examples to compute the cost.\n",
    "</div>\n",
    "<a id=\"nnCostFunction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k, labels/classes\n",
    "\n",
    "m, training examples\n",
    "\n",
    "cost is the summation of all costs: costs of each training example to all k labels, and costs of all training examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input layer has 400 units (+1 bias), \n",
    "\n",
    "input X is (5000, 401) training example x pixels\n",
    "\n",
    "each row are pixel gray scale values for a training example,\n",
    "\n",
    "each column are pixels for all training examples\n",
    "\n",
    "hidden layer has 25 units (+1 bias), \n",
    "\n",
    "weights for input layer is (25, 401), hidden units X input units (+1)\n",
    "\n",
    "each row are weights for a unit in hidden layer\n",
    "\n",
    "each column are weights from an input unit for all units in hidden layer \n",
    "\n",
    "output layer 10 units\n",
    "\n",
    "weights for output layer is (10, 26), output units X hidden units(+1)\n",
    "\n",
    "each row are weights for a unit in output layer\n",
    "\n",
    "each column are weights from a hidden unit for all output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## nnCostFunction implementation notes:\n",
    "\n",
    "### 1) One-hot encoding of y:\n",
    "y_encode is also a 5000x10 matrix, each row is a encoded label for a training iage. If the label is 2, the 3rd element in the row is 1 and the rest 9 elements are all 0s.\n",
    "$$ y = \\begin{bmatrix} 0 & 0 & 1 & 0 \\cdots & 0 \\end{bmatrix}$$\n",
    "\n",
    "### 2) feedforwad outputs (activation of layer 3: a3): \n",
    "a3 is a 5000x10 matrix. Each row is a prediction for a training image. It shows the probability of being each number, 0 to 9. For instance, following y_hat means the predicted digit is 2 with 90% chance and is 0 with 10% probability.\n",
    "$$ y_{hat} = \\begin{bmatrix} 0.1 & 0 & 0.9 & 0 & \\cdots & 0 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "### 3) calculate the cost (loss),\n",
    "The cost function without regularization is:\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right]$$\n",
    "\n",
    "The cost function for neural networks with regularization is given by:\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$\n",
    "use the cost function formula? cost for every class and cost for every training image\n",
    "\n",
    "4) the dimensions do not look right, why does it still work?\n",
    "\n",
    "5) * is element_wise operation, not matrix multiplication @\n",
    "\n",
    "6) for the cost, we only need element-wise multiplication, what is the ituition. when we predicted the exact label at 100% confidence, the cost is 0. when we predict wrong label with 100% confidence, the cost is infinity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    \"\"\"\n",
    "    Implements the neural network cost function and gradient for a two layer neural \n",
    "    network which performs classification. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_params : array_like\n",
    "        The parameters for the neural network which are \"unrolled\" into \n",
    "        a vector. This needs to be converted back into the weight matrices Theta1\n",
    "        and Theta2.\n",
    "    \n",
    "    input_layer_size : int\n",
    "        Number of features for the input layer. \n",
    "    \n",
    "    hidden_layer_size : int\n",
    "        Number of hidden units in the second layer.\n",
    "    \n",
    "    num_labels : int\n",
    "        Total number of labels, or equivalently number of units in output layer. \n",
    "    \n",
    "    X : array_like\n",
    "        Input dataset. A matrix of shape (m x input_layer_size).\n",
    "    \n",
    "    y : array_like\n",
    "        Dataset labels. A vector of shape (m,).\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        Regularization parameter.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function at the current weight values.\n",
    "    \n",
    "    grad : array_like\n",
    "        An \"unrolled\" vector of the partial derivatives of the concatenatation of\n",
    "        neural network weights Theta1 and Theta2.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    You should complete the code by working through the following parts.\n",
    "    \n",
    "    - Part 1: Feedforward the neural network and return the cost in the \n",
    "              variable J. After implementing Part 1, you can verify that your\n",
    "              cost function computation is correct by verifying the cost\n",
    "              computed in the following cell.\n",
    "    \n",
    "    - Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "              Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "              the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "              Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "              that your implementation is correct by running checkNNGradients provided\n",
    "              in the utils.py module.\n",
    "    \n",
    "              Note: The vector y passed into the function is a vector of labels\n",
    "                    containing values from 0..K-1. You need to map this vector into a \n",
    "                    binary vector of 1's and 0's to be used with the neural network\n",
    "                    cost function.\n",
    "     \n",
    "              Hint: We recommend implementing backpropagation using a for-loop\n",
    "                    over the training examples if you are implementing it for the \n",
    "                    first time.\n",
    "    \n",
    "    - Part 3: Implement regularization with the cost function and gradients.\n",
    "    \n",
    "              Hint: You can implement this around the code for\n",
    "                    backpropagation. That is, you can compute the gradients for\n",
    "                    the regularization separately and then add them to Theta1_grad\n",
    "                    and Theta2_grad from Part 2.\n",
    "    \n",
    "    Note \n",
    "    ----\n",
    "    We have provided an implementation for the sigmoid function in the file \n",
    "    `utils.py` accompanying this assignment.\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    # basically this is Theta1 = np.reshape(nn_params[:25*401], 25, 401)\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "    # Theta2 = np.reshape(nn_params[25*401:], 10, 26)\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "    # Setup some useful variables, m is the number of training images\n",
    "    m = y.size\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # onehot encoding y,  y is (m, ) vector\n",
    "    y_encode = np.zeros([m, num_labels])\n",
    "    for i, j in enumerate(y):\n",
    "        y_encode[i, j] = 1\n",
    "\n",
    "    # feedforward to produce the prediction output\n",
    "    bias1 = np.ones((X.shape[0], 1))\n",
    "    X = np.concatenate([bias1, X], axis=1)\n",
    "    # aj is the activation of layer j\n",
    "    a2 = utils.sigmoid(Theta1@(X.T)) \n",
    "    bias2 = np.ones((a2.shape[1], 1))\n",
    "    a2 = np.concatenate([bias2, a2.T], axis=1)\n",
    "    a3 = utils.sigmoid(a2@(Theta2.T))\n",
    "    \n",
    " \n",
    "    J = 1/m * np.sum(-y_encode * np.log(a3) - (1-y_encode)*np.log(1-(a3)))\n",
    "  \n",
    "\n",
    "    # add in regularization, do not regularize bias, which is 1st column of theta1 and teta2\n",
    "    reg = 0.5 * lambda_ /m * (np.sum(np.square(Theta1[:, 1:])) + np.sum(np.square(Theta2[:,1:])))  \n",
    "    J += reg\n",
    "    \n",
    "    print(f'J is {J}')\n",
    "#     print(f'{(y_encode * np.log(output2)).shape}')\n",
    "\n",
    "    # backpropagation, use a loop for each training image here\n",
    "    # a2, a3 are activation for hidden layer and output layer\n",
    "    # ej is detla for layer j, delta1 is accumulative detla for layer1 and delta2 is accumulative delta for layer2 \n",
    "    delta1 = 0\n",
    "    delta2 = 0 \n",
    "    for t in range(m):\n",
    "#         print(f'X shape {X.shape}')\n",
    "        a1 = X[t,:] # X has bias added in during forward passing\n",
    "        a1 = a1[None].T # (401, 1)\n",
    "        z2 = Theta1@(a1)\n",
    "        a2 = utils.sigmoid(z2) #(25, 1) \n",
    "#         print('a1, a2 shape', a1.shape, a2.shape)\n",
    "        bias2 = np.ones((a2.shape[1], 1))\n",
    "        a2 = np.concatenate([bias2, a2], axis=0) # a2 (26, 1)\n",
    "#         print('a1, a2 shape', a1.shape, a2.shape)\n",
    "        z3 = Theta2@(a2)\n",
    "        a3 = utils.sigmoid(z3) # (10, 1)\n",
    "#         print('activation of hidden layer and output layer are:a2, a3', a2.shape, a3.shape)\n",
    "        \n",
    "        # output layer error (e3): e3 = a3 - y, which is the prediction minus the real label.\n",
    "        # y_ecd is y_encode transpose with shape of (10,1)\n",
    "        y_ecd = y_encode[t][None].T # (10,1)\n",
    "#         print('y_encode shape', y_ecd.shape)\n",
    "        e3 = a3 - y_ecd # (10, 1)\n",
    "#         print(f'a3 error is {a3}, y_encode is {y_encode[t].T}, e3 is {e3}', e3.shape)\n",
    "        \n",
    "        # backpropagate error (delta) to hidden layer (l=2), z is the layer 2 activation?\n",
    "        e2 = (Theta2.T@e3) * a2 * (1 - a2) # Theta2 (26, 1)\n",
    "#         e2 = np.squeeze(e2,axis=0)\n",
    "#         e2 = e2[1:] # ignore the error for the bias\n",
    "#         print(f'e2 is {e2}', Theta2.T.shape, e2.shape, e3.shape)\n",
    "    \n",
    "        \n",
    "        \n",
    "        # gradient for the neural network is the accumulated gradient divided by m?:\n",
    "        Theta2_grad = e3 @ (a2.T)\n",
    "        Theta1_grad = e2 @ (a1.T)\n",
    "        \n",
    "        # accumulate the error or delta\n",
    "        delta1 += Theta1_grad\n",
    "        delta2 += Theta2_grad\n",
    "        \n",
    "    Theta1_grad = delta1/m\n",
    "    Theta2_grad = delta2/m\n",
    "    print(f'grad is ', Theta1_grad.shape, Theta2_grad.shape, delta1.shape, delta2.shape)\n",
    "   \n",
    "        \n",
    "    \n",
    "    \n",
    "    # ================================================================\n",
    "    # Unroll gradients\n",
    "#     grad = np.concatenate([Theta1_grad.ravel(order=order), Theta2_grad.ravel(order=order)])\n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from: https://towardsdatascience.com/andrew-ngs-machine-learning-course-in-python-neural-networks-e526b41fdcd9\n",
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    computes the gradient of the sigmoid function\n",
    "    \"\"\"\n",
    "    sigmoid = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    return sigmoid *(1-sigmoid)\n",
    "\n",
    "def nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,lambda_=0.0):\n",
    "    \"\"\"\n",
    "    nn_params contains the parameters unrolled into a vector\n",
    "    \n",
    "    compute the cost and gradient of the neural network\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2\n",
    "    Theta1 = nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "    Theta2 = nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    J=0\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    y_encode = np.zeros((m,num_labels))\n",
    "    for i, j in enumerate(y):\n",
    "        y_encode[i, j] = 1\n",
    "    \n",
    "    a1 = utils.sigmoid(X @ Theta1.T)\n",
    "    a1 = np.hstack((np.ones((m,1)), a1)) # hidden layer\n",
    "    a2 = utils.sigmoid(a1 @ Theta2.T) # output layer\n",
    "    \n",
    "#     for i in range(1,num_labels+1):\n",
    "#         y_encode[:,i-1][:,np.newaxis] = np.where(y==i,1,0)\n",
    "    for j in range(num_labels):\n",
    "        J = J + sum(-y_encode[:,j] * np.log(a2[:,j]) - (1-y_encode[:,j])*np.log(1-a2[:,j]))\n",
    "    \n",
    "    cost = 1/m* J\n",
    "    reg_J = cost + lambda_/(2*m) * (np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:,1:]**2))\n",
    "    \n",
    "    # Implement the backpropagation algorithm to compute the gradients\n",
    "    \n",
    "    Theta1_grad = np.zeros((Theta1.shape))\n",
    "    Theta2_grad = np.zeros((Theta2.shape))\n",
    "    \n",
    "    for i in range(m):\n",
    "        xi= X[i,:] # 1 X 401\n",
    "        a1i = a1[i,:] # 1 X 26\n",
    "        a2i =a2[i,:] # 1 X 10\n",
    "        d2 = a2i - y_encode[i,:]\n",
    "        d1 = Theta2.T @ d2.T * sigmoidGradient(np.hstack((1,xi @ Theta1.T)))\n",
    "        Theta1_grad= Theta1_grad + d1[1:][:,np.newaxis] @ xi[:,np.newaxis].T\n",
    "        Theta2_grad = Theta2_grad + d2.T[:,np.newaxis] @ a1i[:,np.newaxis].T\n",
    "        \n",
    "    Theta1_grad = 1/m * Theta1_grad\n",
    "    Theta2_grad = 1/m*Theta2_grad\n",
    "    \n",
    "    Theta1_grad_reg = Theta1_grad + (lambda_/m) * np.hstack((np.zeros((Theta1.shape[0],1)),Theta1[:,1:]))\n",
    "    Theta2_grad_reg = Theta2_grad + (lambda_/m) * np.hstack((np.zeros((Theta2.shape[0],1)),Theta2[:,1:]))\n",
    "    grad = np.concatenate([Theta1_grad_reg.ravel(), Theta2_grad_reg.ravel()])\n",
    "\n",
    "    return reg_J, grad\n",
    "#     return cost, Theta1_grad, Theta2_grad,reg_J, Theta1_grad_reg,Theta2_grad_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modify the implementation myself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function squashes input (a number) into a value, which is between 0 and 1. We use sigmoid function as activation function because we would like to constrain the activation. When it is close to 1, we are saying the unit is activated. When is close to 0, we are saying the unit is not activated.\n",
    "\n",
    "Sigmoid function is given by this formula:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{e^{z}}{e^{z} + 1}\n",
    "$$\n",
    "\n",
    "The gradient for the sigmoid function can be computed as following:\n",
    "$$\n",
    "\\sigma'(z) = \\frac{\\partial \\sigma}{\\partial z} = \\sigma(z)(1 - \\sigma(z))\n",
    "$$\n",
    "\n",
    "The intuition is that the gradient of a sigmoid function is equal to the sigmoid function times 1 minus the sigmoid function iteself().\n",
    "\n",
    "\n",
    "\n",
    "Now complete the implementation of `sigmoidGradient` in the next cell.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    computes the gradient of the sigmoid function\n",
    "    \"\"\"\n",
    "    sigmoid = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    return sigmoid *(1-sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,lambda_=0.0):\n",
    "    \"\"\"\n",
    "    nn_params contains the parameters unrolled into a vector\n",
    "    \n",
    "    compute the cost and gradient of the neural network\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2\n",
    "    Theta1 = nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)\n",
    "    Theta2 = nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    J=0\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "    y_encode = np.zeros((m,num_labels))\n",
    "    for i, j in enumerate(y):\n",
    "        y_encode[i, j] = 1\n",
    "    \n",
    "    a1 = utils.sigmoid(X @ Theta1.T)\n",
    "    a1 = np.hstack((np.ones((m,1)), a1)) # hidden layer\n",
    "    a2 = utils.sigmoid(a1 @ Theta2.T) # output layer\n",
    "    \n",
    "#     for i in range(1,num_labels+1):\n",
    "#         y_encode[:,i-1][:,np.newaxis] = np.where(y==i,1,0)\n",
    "    for j in range(num_labels):\n",
    "        J = J + sum(-y_encode[:,j] * np.log(a2[:,j]) - (1-y_encode[:,j])*np.log(1-a2[:,j]))\n",
    "    \n",
    "    cost = 1/m* J\n",
    "    reg_J = cost + lambda_/(2*m) * (np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:,1:]**2))\n",
    "    \n",
    "    # Implement the backpropagation algorithm to compute the gradients\n",
    "    \n",
    "    Theta1_grad = np.zeros((Theta1.shape))\n",
    "    Theta2_grad = np.zeros((Theta2.shape))\n",
    "    \n",
    "    for i in range(1):\n",
    "        print('training example:', i)\n",
    "        xi= X[i,:] # 1 X 401\n",
    "        a1i = a1[i,:] # 1 X 26\n",
    "        a2i =a2[i,:] # 1 X 10\n",
    "        delta2 = a2i - y_encode[i,:]\n",
    "        print('a2i, y_encode[i,:]', a2i, y_encode[i,:], delta2)\n",
    "        delta1 = Theta2.T @ delta2.T * sigmoidGradient(np.hstack((1,xi @ Theta1.T)))\n",
    "        Theta1_grad= Theta1_grad + delta1[1:][:,np.newaxis] @ xi[:,np.newaxis].T\n",
    "        Theta2_grad = Theta2_grad + delta2.T[:,np.newaxis] @ a1i[:,np.newaxis].T\n",
    "\n",
    "        \n",
    "    Theta1_grad = 1/m * Theta1_grad\n",
    "    Theta2_grad = 1/m*Theta2_grad\n",
    "    \n",
    "    Theta1_grad_reg = Theta1_grad + (lambda_/m) * np.hstack((np.zeros((Theta1.shape[0],1)),Theta1[:,1:]))\n",
    "    Theta2_grad_reg = Theta2_grad + (lambda_/m) * np.hstack((np.zeros((Theta2.shape[0],1)),Theta2[:,1:]))\n",
    "    grad = np.concatenate([Theta1_grad_reg.ravel(), Theta2_grad_reg.ravel()])\n",
    "\n",
    "    return reg_J, grad\n",
    "#     return cost, Theta1_grad, Theta2_grad,reg_J, Theta1_grad_reg,Theta2_grad_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training example: 0\n",
      "a2i, y_encode[i,:] [9.95734012e-01 1.12661530e-04 1.74127856e-03 2.52696959e-03\n",
      " 1.84032321e-05 9.36263860e-03 3.99270267e-03 5.51517524e-03\n",
      " 4.01468105e-04 6.48072305e-03] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [-4.26598801e-03  1.12661530e-04  1.74127856e-03  2.52696959e-03\n",
      "  1.84032321e-05  9.36263860e-03  3.99270267e-03  5.51517524e-03\n",
      "  4.01468105e-04  6.48072305e-03]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.28762916516131876,\n",
       " array([-5.12078395e-08,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         1.26052392e-06,  1.22281085e-06,  1.38964573e-07]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., 99.,  0.],\n",
       "       [ 0., 99.,  0.,  0.],\n",
       "       [99.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Toy example of one-hot encoding\n",
    "lbs = np.zeros([3,4])\n",
    "lbs\n",
    "# mutate the values at specific position in the array\n",
    "for i, j in enumerate([2,1,0]):\n",
    "    lbs[i,j] = 99\n",
    "lbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "Use the following links to go back to the different parts of this exercise that require to modify the function `nnCostFunction`.<br>\n",
    "\n",
    "Back to:\n",
    "- [Feedforward and cost function](#section1)\n",
    "- [Regularized cost](#section2)\n",
    "- [Neural Network Gradient (Backpropagation)](#section4)\n",
    "- [Regularized Gradient](#section5)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done, call your `nnCostFunction` using the loaded set of parameters for `Theta1` and `Theta2`. You should see that the cost is about 0.287629."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 0.287629 \n",
      "The cost should be about                   : 0.287629.\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, X, y, lambda_)\n",
    "print('Cost at parameters (loaded from ex4weights): %.6f ' % J)\n",
    "print('The cost should be about                   : 0.287629.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submitting Solutions | Programming Exercise neural-network-learning\n",
      "\n",
      "Use token from last successful submission (szong@bcgsc.ca)? (Y/n): y\n",
      "                                  Part Name |     Score | Feedback\n",
      "                                  --------- |     ----- | --------\n",
      "              Feedforward and Cost Function |  30 /  30 | Nice work!\n",
      "                  Regularized Cost Function |   0 /  15 | \n",
      "                           Sigmoid Gradient |   0 /   5 | \n",
      "  Neural Network Gradient (Backpropagation) |   0 /  40 | \n",
      "                       Regularized Gradient |   0 /  10 | \n",
      "                                  --------------------------------\n",
      "                                            |  30 / 100 |  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grader = utils.Grader()\n",
    "grader[1] = nnCostFunction\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "### 1.4 Regularized cost function\n",
    "\n",
    "The cost function for neural networks with regularization is given by:\n",
    "\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$\n",
    "\n",
    "You can assume that the neural network will only have 3 layers - an input layer, a hidden layer and an output layer. However, your code should work for any number of input units, hidden units and outputs units. While we\n",
    "have explicitly listed the indices above for $\\Theta^{(1)}$ and $\\Theta^{(2)}$ for clarity, do note that your code should in general work with $\\Theta^{(1)}$ and $\\Theta^{(2)}$ of any size. Note that you should not be regularizing the terms that correspond to the bias. For the matrices `Theta1` and `Theta2`, this corresponds to the first column of each matrix. You should now add regularization to your cost function. Notice that you can first compute the unregularized cost function $J$ using your existing `nnCostFunction` and then later add the cost for the regularization terms.\n",
    "\n",
    "[Click here to go back to `nnCostFunction` for editing.](#nnCostFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done, the next cell will call your `nnCostFunction` using the loaded set of parameters for `Theta1` and `Theta2`, and $\\lambda = 1$. You should see that the cost is about 0.383770."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from ex4weights): 0.383770\n",
      "This value should be about                 : 0.383770.\n"
     ]
    }
   ],
   "source": [
    "# Weight regularization parameter (we set this to 1 here).\n",
    "lambda_ = 1\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                      num_labels, X, y, lambda_)\n",
    "\n",
    "print('Cost at parameters (loaded from ex4weights): %.6f' % J)\n",
    "print('This value should be about                 : 0.383770.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submitting Solutions | Programming Exercise neural-network-learning\n",
      "\n",
      "Use token from last successful submission (szong@bcgsc.ca)? (Y/n): y\n",
      "                                  Part Name |     Score | Feedback\n",
      "                                  --------- |     ----- | --------\n",
      "              Feedforward and Cost Function |  30 /  30 | Nice work!\n",
      "                  Regularized Cost Function |  15 /  15 | Nice work!\n",
      "                           Sigmoid Gradient |   0 /   5 | \n",
      "  Neural Network Gradient (Backpropagation) |   0 /  40 | \n",
      "                       Regularized Gradient |   0 /  10 | \n",
      "                                  --------------------------------\n",
      "                                            |  45 / 100 |  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grader[2] = nnCostFunction\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Backpropagation\n",
    "\n",
    "In this part of the exercise, you will implement the backpropagation algorithm to compute the gradient for the neural network cost function. You will need to update the function `nnCostFunction` so that it returns an appropriate value for `grad`. Once you have computed the gradient, you will be able to train the neural network by minimizing the cost function $J(\\theta)$ using an advanced optimizer such as `scipy`'s `optimize.minimize`.\n",
    "You will first implement the backpropagation algorithm to compute the gradients for the parameters for the (unregularized) neural network. After you have verified that your gradient computation for the unregularized case is correct, you will implement the gradient for the regularized neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "### 2.1 Sigmoid Gradient\n",
    "\n",
    "To help you get started with this part of the exercise, you will first implement\n",
    "the sigmoid gradient function. The gradient for the sigmoid function can be\n",
    "computed as\n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz} g(z) = g(z)\\left(1-g(z)\\right) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\text{sigmoid}(z) = g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Now complete the implementation of `sigmoidGradient` in the next cell.\n",
    "<a id=\"sigmoidGradient\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sigmoid function evaluated at z. \n",
    "    This should work regardless if z is a matrix or a vector. \n",
    "    In particular, if z is a vector or matrix, you should return\n",
    "    the gradient for each element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A vector or matrix as input to the sigmoid function. \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    g : array_like\n",
    "        Gradient of the sigmoid function. Has the same shape as z. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the gradient of the sigmoid function evaluated at\n",
    "    each value of z (z can be a matrix, vector or scalar).\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    We have provided an implementation of the sigmoid function \n",
    "    in `utils.py` file accompanying this assignment.\n",
    "    \"\"\"\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    sig = utils.sigmoid(z)    \n",
    "    g = sig * (1 - sig )\n",
    "    print(f'g is {g}')\n",
    "\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, the following cell call `sigmoidGradient` on a given vector `z`. Try testing a few values by calling `sigmoidGradient(z)`. For large values (both positive and negative) of z, the gradient should be close to 0. When $z = 0$, the gradient should be exactly 0.25. Your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid gradient function on every element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g is [0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n",
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "z = np.array([-1, -0.5, 0, 0.5, 1])\n",
    "g = sigmoidGradient(z)\n",
    "print('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submitting Solutions | Programming Exercise neural-network-learning\n",
      "\n",
      "Use token from last successful submission (szong@bcgsc.ca)? (Y/n): y\n",
      "g is [0.19661193 0.18777432 0.24980417 0.19211211 0.20295431]\n",
      "g is [0.19661193 0.19102541 0.24975014 0.19586787 0.20529662]\n",
      "g is [0.19661193 0.21191842 0.24914071 0.20358273 0.22534952]\n",
      "g is [0.19661193 0.22854698 0.24371366 0.20809992 0.24006791]\n",
      "g is [0.19661193 0.22868009 0.24215857 0.20554301 0.24069846]\n",
      "g is [0.19661193 0.2122605  0.24759642 0.19807138 0.22728766]\n",
      "g is [0.19661193 0.19125588 0.24998471 0.1924604  0.20673942]\n",
      "g is [0.19661193 0.18763784 0.24956062 0.19411725 0.20209057]\n",
      "g is [0.19661193 0.20542331 0.24981288 0.2014245  0.21921376]\n",
      "g is [0.19661193 0.22534726 0.24540647 0.20747803 0.23737892]\n",
      "g is [0.19661193 0.23041339 0.24164481 0.2069736  0.24185186]\n",
      "g is [0.19661193 0.21827252 0.24593096 0.20033426 0.23254249]\n",
      "g is [0.19661193 0.19627498 0.24991355 0.19344218 0.21194618]\n",
      "g is [0.19661193 0.1861395  0.24953169 0.19285769 0.20083742]\n",
      "g is [0.19661193 0.19907353 0.24999993 0.19915185 0.21309865]\n",
      "g is [0.19661193 0.22083435 0.24712698 0.20629186 0.23346195]\n",
      "g is [0.19661193 0.18777432 0.24980417 0.19211211 0.20295431]\n",
      "g is [0.19661193 0.19102541 0.24975014 0.19586787 0.20529662]\n",
      "g is [0.19661193 0.21191842 0.24914071 0.20358273 0.22534952]\n",
      "g is [0.19661193 0.22854698 0.24371366 0.20809992 0.24006791]\n",
      "g is [0.19661193 0.22868009 0.24215857 0.20554301 0.24069846]\n",
      "g is [0.19661193 0.2122605  0.24759642 0.19807138 0.22728766]\n",
      "g is [0.19661193 0.19125588 0.24998471 0.1924604  0.20673942]\n",
      "g is [0.19661193 0.18763784 0.24956062 0.19411725 0.20209057]\n",
      "g is [0.19661193 0.20542331 0.24981288 0.2014245  0.21921376]\n",
      "g is [0.19661193 0.22534726 0.24540647 0.20747803 0.23737892]\n",
      "g is [0.19661193 0.23041339 0.24164481 0.2069736  0.24185186]\n",
      "g is [0.19661193 0.21827252 0.24593096 0.20033426 0.23254249]\n",
      "g is [0.19661193 0.19627498 0.24991355 0.19344218 0.21194618]\n",
      "g is [0.19661193 0.1861395  0.24953169 0.19285769 0.20083742]\n",
      "g is [0.19661193 0.19907353 0.24999993 0.19915185 0.21309865]\n",
      "g is [0.19661193 0.22083435 0.24712698 0.20629186 0.23346195]\n",
      "g is [[0.06866401 0.08484199 0.10733201 0.13679999 0.17209629 0.20861413\n",
      "  0.23777833 0.24995594 0.24039993 0.21287124]\n",
      " [0.05758392 0.05047142 0.04649902 0.04517786 0.04634204 0.05013784\n",
      "  0.05703332 0.06783488 0.08365743 0.10572272]\n",
      " [0.23912422 0.21075806 0.17441405 0.13886289 0.10896913 0.08605137\n",
      "  0.06951302 0.05814991 0.05081717 0.04666625]]\n",
      "                                  Part Name |     Score | Feedback\n",
      "                                  --------- |     ----- | --------\n",
      "              Feedforward and Cost Function |  30 /  30 | Nice work!\n",
      "                  Regularized Cost Function |  15 /  15 | Nice work!\n",
      "                           Sigmoid Gradient |   5 /   5 | Nice work!\n",
      "  Neural Network Gradient (Backpropagation) |   0 /  40 | \n",
      "                       Regularized Gradient |   0 /  10 | \n",
      "                                  --------------------------------\n",
      "                                            |  50 / 100 |  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grader[3] = sigmoidGradient\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Random Initialization\n",
    "\n",
    "When training neural networks, it is important to randomly initialize the parameters for symmetry breaking. One effective strategy for random initialization is to randomly select values for $\\Theta^{(l)}$ uniformly in the range $[-\\epsilon_{init}, \\epsilon_{init}]$. You should use $\\epsilon_{init} = 0.12$. This range of values ensures that the parameters are kept small and makes the learning more efficient.\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "One effective strategy for choosing $\\epsilon_{init}$ is to base it on the number of units in the network. A good choice of $\\epsilon_{init}$ is $\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}$ where $L_{in} = s_l$ and $L_{out} = s_{l+1}$ are the number of units in the layers adjacent to $\\Theta^{l}$.\n",
    "</div>\n",
    "\n",
    "Your job is to complete the function `randInitializeWeights` to initialize the weights for $\\Theta$. Modify the function by filling in the following code:\n",
    "\n",
    "```python\n",
    "# Randomly initialize the weights to small values\n",
    "W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "```\n",
    "Note that we give the function an argument for $\\epsilon$ with default value `epsilon_init = 0.12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
    "    \"\"\"\n",
    "    Randomly initialize the weights of a layer in a neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L_in : int\n",
    "        Number of incomming connections.\n",
    "    \n",
    "    L_out : int\n",
    "        Number of outgoing connections. \n",
    "    \n",
    "    epsilon_init : float, optional\n",
    "        Range of values which the weight can take from a uniform \n",
    "        distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W : array_like\n",
    "        The weight initialiatized to random values.  Note that W should\n",
    "        be set to a matrix of size(L_out, 1 + L_in) as\n",
    "        the first column of W handles the \"bias\" terms.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Initialize W randomly so that we break the symmetry while training\n",
    "    the neural network. Note that the first column of W corresponds \n",
    "    to the parameters for the bias unit.\n",
    "    \"\"\"\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "    # ============================================================\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You do not need to submit any code for this part of the exercise.*\n",
    "\n",
    "Execute the following cell to initialize the weights for the 2 layers in the neural network using the `randInitializeWeights` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Neural Network Parameters ...\n"
     ]
    }
   ],
   "source": [
    "print('Initializing Neural Network Parameters ...')\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "# Unroll parameters\n",
    "initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 401)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_Theta1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "### 2.4 Backpropagation\n",
    "\n",
    "![](Figures/ex4-backpropagation.png)\n",
    "\n",
    "Now, you will implement the backpropagation algorithm. Recall that the intuition behind the backpropagation algorithm is as follows. Given a training example $(x^{(t)}, y^{(t)})$, we will first run a “forward pass” to compute all the activations throughout the network, including the output value of the hypothesis $h_\\theta(x)$. Then, for each node $j$ in layer $l$, we would like to compute an “error term” $\\delta_j^{(l)}$ that measures how much that node was “responsible” for any errors in our output.\n",
    "\n",
    "For an output node, we can directly measure the difference between the network’s activation and the true target value, and use that to define $\\delta_j^{(3)}$ (since layer 3 is the output layer). For the hidden units, you will compute $\\delta_j^{(l)}$ based on a weighted average of the error terms of the nodes in layer $(l+1)$. In detail, here is the backpropagation algorithm (also depicted in the figure above). You should implement steps 1 to 4 in a loop that processes one example at a time. Concretely, you should implement a for-loop `for t in range(m)` and place steps 1-4 below inside the for-loop, with the $t^{th}$ iteration performing the calculation on the $t^{th}$ training example $(x^{(t)}, y^{(t)})$. Step 5 will divide the accumulated gradients by $m$ to obtain the gradients for the neural network cost function.\n",
    "\n",
    "1. Set the input layer’s values $(a^{(1)})$ to the $t^{th }$training example $x^{(t)}$. Perform a feedforward pass, computing the activations $(z^{(2)}, a^{(2)}, z^{(3)}, a^{(3)})$ for layers 2 and 3. Note that you need to add a `+1` term to ensure that the vectors of activations for layers $a^{(1)}$ and $a^{(2)}$ also include the bias unit. In `numpy`, if a 1 is a column matrix, adding one corresponds to `a_1 = np.concatenate([np.ones((m, 1)), a_1], axis=1)`.\n",
    "\n",
    "1. For each output unit $k$ in layer 3 (the output layer), set \n",
    "$$\\delta_k^{(3)} = \\left(a_k^{(3)} - y_k \\right)$$\n",
    "where $y_k \\in \\{0, 1\\}$ indicates whether the current training example belongs to class $k$ $(y_k = 1)$, or if it belongs to a different class $(y_k = 0)$. You may find logical arrays helpful for this task (explained in the previous programming exercise).\n",
    "\n",
    "1. For the hidden layer $l = 2$, set \n",
    "$$ \\delta^{(2)} = \\left( \\Theta^{(2)} \\right)^T \\delta^{(3)} * g'\\left(z^{(2)} \\right)$$\n",
    "Note that the symbol $*$ performs element wise multiplication in `numpy`.\n",
    "\n",
    "1. Accumulate the gradient from this example using the following formula. Note that you should skip or remove $\\delta_0^{(2)}$. In `numpy`, removing $\\delta_0^{(2)}$ corresponds to `delta_2 = delta_2[1:]`.\n",
    "\n",
    "1. Obtain the (unregularized) gradient for the neural network cost function by dividing the accumulated gradients by $\\frac{1}{m}$:\n",
    "$$ \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)}$$\n",
    "\n",
    "<div class=\"alert alert-box alert-warning\">\n",
    "**Python/Numpy tip**: You should implement the backpropagation algorithm only after you have successfully completed the feedforward and cost functions. While implementing the backpropagation alogrithm, it is often useful to use the `shape` function to print out the shapes of the variables you are working with if you run into dimension mismatch errors.\n",
    "</div>\n",
    "\n",
    "[Click here to go back and update the function `nnCostFunction` with the backpropagation algorithm](#nnCostFunction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have implemented the backpropagation algorithm, we will proceed to run gradient checking on your implementation. The gradient check will allow you to increase your confidence that your code is\n",
    "computing the gradients correctly.\n",
    "\n",
    "### 2.4  Gradient checking \n",
    "\n",
    "In your neural network, you are minimizing the cost function $J(\\Theta)$. To perform gradient checking on your parameters, you can imagine “unrolling” the parameters $\\Theta^{(1)}$, $\\Theta^{(2)}$ into a long vector $\\theta$. By doing so, you can think of the cost function being $J(\\Theta)$ instead and use the following gradient checking procedure.\n",
    "\n",
    "Suppose you have a function $f_i(\\theta)$ that purportedly computes $\\frac{\\partial}{\\partial \\theta_i} J(\\theta)$; you’d like to check if $f_i$ is outputting correct derivative values.\n",
    "\n",
    "$$\n",
    "\\text{Let } \\theta^{(i+)} = \\theta + \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "\\quad \\text{and} \\quad \\theta^{(i-)} = \\theta - \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ \\epsilon \\\\ \\vdots \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So, $\\theta^{(i+)}$ is the same as $\\theta$, except its $i^{th}$ element has been incremented by $\\epsilon$. Similarly, $\\theta^{(i−)}$ is the corresponding vector with the $i^{th}$ element decreased by $\\epsilon$. You can now numerically verify $f_i(\\theta)$’s correctness by checking, for each $i$, that:\n",
    "\n",
    "$$ f_i\\left( \\theta \\right) \\approx \\frac{J\\left( \\theta^{(i+)}\\right) - J\\left( \\theta^{(i-)} \\right)}{2\\epsilon} $$\n",
    "\n",
    "The degree to which these two values should approximate each other will depend on the details of $J$. But assuming $\\epsilon = 10^{-4}$, you’ll usually find that the left- and right-hand sides of the above will agree to at least 4 significant digits (and often many more).\n",
    "\n",
    "We have implemented the function to compute the numerical gradient for you in `computeNumericalGradient` (within the file `utils.py`). While you are not required to modify the file, we highly encourage you to take a look at the code to understand how it works.\n",
    "\n",
    "In the next cell we will run the provided function `checkNNGradients` which will create a small neural network and dataset that will be used for checking your gradients. If your backpropagation implementation is correct,\n",
    "you should see a relative difference that is less than 1e-9.\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "**Practical Tip**: When performing gradient checking, it is much more efficient to use a small neural network with a relatively small number of input units and hidden units, thus having a relatively small number\n",
    "of parameters. Each dimension of $\\theta$ requires two evaluations of the cost function and this can be expensive. In the function `checkNNGradients`, our code creates a small random model and dataset which is used with `computeNumericalGradient` for gradient checking. Furthermore, after you are confident that your gradient computations are correct, you should turn off gradient checking before running your learning algorithm.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "**Practical Tip:** Gradient checking works for any function where you are computing the cost and the gradient. Concretely, you can use the same `computeNumericalGradient` function to check if your gradient implementations for the other exercises are correct too (e.g., logistic regression’s cost function).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952352 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963477 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967132 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960848 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949112 0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952134 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963286 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24966951 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.2496065  0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24948887 0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952252 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.2496339  0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967043 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960742 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24948989 0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952234 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963373 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.2496704  0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960757 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949011 0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.2495224  0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963388 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967051 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960753 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24948994 0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952246 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963375 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967033 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960745 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949006 0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952232 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963376 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967046 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960759 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949007 0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952254 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963386 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967038 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.2496074  0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24948992 0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927951 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939536 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.24957603 0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.24965693 0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958839 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927683 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.2493929  0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.24957397 0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.24965508 0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958637 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927828 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939424 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.24957502 0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.24965593 0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958728 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927806 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939402 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.24957499 0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.24965607 0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958748 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927813 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939421 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.2495751  0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.24965604 0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958732 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927821 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939405 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.2495749  0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.24965597 0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958744 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927804 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939407 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.24957504 0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.24965609 0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958745 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.2492783  0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.2493942  0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.24957496 0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.24965591 0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958731 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994862 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994451 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998317 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999996 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.24999956 0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.2499479  0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994377 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998276 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999994 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.24999963 0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994829 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994417 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994823 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994411 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998296 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.24999959 0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994825 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994416 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998299 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994827 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994411 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998294 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.24999959 0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994823 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994412 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.24999959 0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.2499483  0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994416 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998296 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963384 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972523 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.249738   0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.2496642  0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955798 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963574 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972688 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973961 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966603 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24956008 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963471 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972598 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.2497388  0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966519 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955913 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963487 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972613 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973882 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966505 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955893 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963482 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.249726   0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973873 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966508 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955909 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963476 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972611 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973889 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966515 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955897 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963489 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.2497261  0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973877 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966503 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955896 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.2496347  0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972601 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973884 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966521 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.2495591  0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922436]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935638]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953304]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959891]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951248]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922714]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935891]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.2495352 ]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24960091]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951468]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922563]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935753]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953411]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959999]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951369]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922587]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935776]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953414]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959984]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951348]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922579]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935757]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953401]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959987]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951364]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922571]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935773]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953423]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959995]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951352]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922589]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935772]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953408]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959981]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951351]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922561]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935758]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953417]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24960001]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951366]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "g is [0.19661193 0.24952243 0.24927817 0.24994826 0.24963479 0.24922575]\n",
      "g is [0.19661193 0.24963381 0.24939413 0.24994414 0.24972606 0.24935765]\n",
      "g is [0.19661193 0.24967042 0.249575   0.24998297 0.24973881 0.24953412]\n",
      "g is [0.19661193 0.24960749 0.249656   0.24999995 0.24966512 0.24959991]\n",
      "g is [0.19661193 0.24949    0.24958738 0.2499996  0.24955903 0.24951358]\n",
      "[[-9.27825235e-03 -9.27825236e-03]\n",
      " [-3.04978931e-06 -3.04978914e-06]\n",
      " [-1.75060084e-04 -1.75060082e-04]\n",
      " [-9.62660640e-05 -9.62660620e-05]\n",
      " [ 8.89911959e-03  8.89911960e-03]\n",
      " [ 1.42869450e-05  1.42869443e-05]\n",
      " [ 2.33146356e-04  2.33146357e-04]\n",
      " [ 1.17982666e-04  1.17982666e-04]\n",
      " [-8.36010761e-03 -8.36010762e-03]\n",
      " [-2.59383093e-05 -2.59383100e-05]\n",
      " [-2.87468729e-04 -2.87468729e-04]\n",
      " [-1.37149705e-04 -1.37149706e-04]\n",
      " [ 7.62813550e-03  7.62813551e-03]\n",
      " [ 3.69883213e-05  3.69883234e-05]\n",
      " [ 3.35320347e-04  3.35320347e-04]\n",
      " [ 1.53247079e-04  1.53247082e-04]\n",
      " [-6.74798370e-03 -6.74798370e-03]\n",
      " [-4.68759787e-05 -4.68759769e-05]\n",
      " [-3.76215588e-04 -3.76215587e-04]\n",
      " [-1.66560294e-04 -1.66560294e-04]\n",
      " [ 3.14544970e-01  3.14544970e-01]\n",
      " [ 1.64090819e-01  1.64090819e-01]\n",
      " [ 1.64567932e-01  1.64567932e-01]\n",
      " [ 1.58339334e-01  1.58339334e-01]\n",
      " [ 1.51127527e-01  1.51127527e-01]\n",
      " [ 1.49568335e-01  1.49568335e-01]\n",
      " [ 1.11056588e-01  1.11056588e-01]\n",
      " [ 5.75736493e-02  5.75736493e-02]\n",
      " [ 5.77867378e-02  5.77867378e-02]\n",
      " [ 5.59235296e-02  5.59235296e-02]\n",
      " [ 5.36967009e-02  5.36967009e-02]\n",
      " [ 5.31542052e-02  5.31542052e-02]\n",
      " [ 9.74006970e-02  9.74006970e-02]\n",
      " [ 5.04575855e-02  5.04575855e-02]\n",
      " [ 5.07530173e-02  5.07530173e-02]\n",
      " [ 4.91620841e-02  4.91620841e-02]\n",
      " [ 4.71456249e-02  4.71456249e-02]\n",
      " [ 4.65597186e-02  4.65597186e-02]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 2.33553e-11\n"
     ]
    }
   ],
   "source": [
    "utils.checkNNGradients(nnCostFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Once your cost function passes the gradient check for the (unregularized) neural network cost function, you should submit the neural network gradient function (backpropagation).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submitting Solutions | Programming Exercise neural-network-learning\n",
      "\n",
      "Use token from last successful submission (szong@bcgsc.ca)? (Y/n): y\n",
      "g is [0.19661193 0.18777432 0.24980417 0.19211211 0.20295431]\n",
      "g is [0.19661193 0.19102541 0.24975014 0.19586787 0.20529662]\n",
      "g is [0.19661193 0.21191842 0.24914071 0.20358273 0.22534952]\n",
      "g is [0.19661193 0.22854698 0.24371366 0.20809992 0.24006791]\n",
      "g is [0.19661193 0.22868009 0.24215857 0.20554301 0.24069846]\n",
      "g is [0.19661193 0.2122605  0.24759642 0.19807138 0.22728766]\n",
      "g is [0.19661193 0.19125588 0.24998471 0.1924604  0.20673942]\n",
      "g is [0.19661193 0.18763784 0.24956062 0.19411725 0.20209057]\n",
      "g is [0.19661193 0.20542331 0.24981288 0.2014245  0.21921376]\n",
      "g is [0.19661193 0.22534726 0.24540647 0.20747803 0.23737892]\n",
      "g is [0.19661193 0.23041339 0.24164481 0.2069736  0.24185186]\n",
      "g is [0.19661193 0.21827252 0.24593096 0.20033426 0.23254249]\n",
      "g is [0.19661193 0.19627498 0.24991355 0.19344218 0.21194618]\n",
      "g is [0.19661193 0.1861395  0.24953169 0.19285769 0.20083742]\n",
      "g is [0.19661193 0.19907353 0.24999993 0.19915185 0.21309865]\n",
      "g is [0.19661193 0.22083435 0.24712698 0.20629186 0.23346195]\n",
      "g is [0.19661193 0.18777432 0.24980417 0.19211211 0.20295431]\n",
      "g is [0.19661193 0.19102541 0.24975014 0.19586787 0.20529662]\n",
      "g is [0.19661193 0.21191842 0.24914071 0.20358273 0.22534952]\n",
      "g is [0.19661193 0.22854698 0.24371366 0.20809992 0.24006791]\n",
      "g is [0.19661193 0.22868009 0.24215857 0.20554301 0.24069846]\n",
      "g is [0.19661193 0.2122605  0.24759642 0.19807138 0.22728766]\n",
      "g is [0.19661193 0.19125588 0.24998471 0.1924604  0.20673942]\n",
      "g is [0.19661193 0.18763784 0.24956062 0.19411725 0.20209057]\n",
      "g is [0.19661193 0.20542331 0.24981288 0.2014245  0.21921376]\n",
      "g is [0.19661193 0.22534726 0.24540647 0.20747803 0.23737892]\n",
      "g is [0.19661193 0.23041339 0.24164481 0.2069736  0.24185186]\n",
      "g is [0.19661193 0.21827252 0.24593096 0.20033426 0.23254249]\n",
      "g is [0.19661193 0.19627498 0.24991355 0.19344218 0.21194618]\n",
      "g is [0.19661193 0.1861395  0.24953169 0.19285769 0.20083742]\n",
      "g is [0.19661193 0.19907353 0.24999993 0.19915185 0.21309865]\n",
      "g is [0.19661193 0.22083435 0.24712698 0.20629186 0.23346195]\n",
      "g is [[0.06866401 0.08484199 0.10733201 0.13679999 0.17209629 0.20861413\n",
      "  0.23777833 0.24995594 0.24039993 0.21287124]\n",
      " [0.05758392 0.05047142 0.04649902 0.04517786 0.04634204 0.05013784\n",
      "  0.05703332 0.06783488 0.08365743 0.10572272]\n",
      " [0.23912422 0.21075806 0.17441405 0.13886289 0.10896913 0.08605137\n",
      "  0.06951302 0.05814991 0.05081717 0.04666625]]\n",
      "g is [0.19661193 0.18777432 0.24980417 0.19211211 0.20295431]\n",
      "g is [0.19661193 0.19102541 0.24975014 0.19586787 0.20529662]\n",
      "g is [0.19661193 0.21191842 0.24914071 0.20358273 0.22534952]\n",
      "g is [0.19661193 0.22854698 0.24371366 0.20809992 0.24006791]\n",
      "g is [0.19661193 0.22868009 0.24215857 0.20554301 0.24069846]\n",
      "g is [0.19661193 0.2122605  0.24759642 0.19807138 0.22728766]\n",
      "g is [0.19661193 0.19125588 0.24998471 0.1924604  0.20673942]\n",
      "g is [0.19661193 0.18763784 0.24956062 0.19411725 0.20209057]\n",
      "g is [0.19661193 0.20542331 0.24981288 0.2014245  0.21921376]\n",
      "g is [0.19661193 0.22534726 0.24540647 0.20747803 0.23737892]\n",
      "g is [0.19661193 0.23041339 0.24164481 0.2069736  0.24185186]\n",
      "g is [0.19661193 0.21827252 0.24593096 0.20033426 0.23254249]\n",
      "g is [0.19661193 0.19627498 0.24991355 0.19344218 0.21194618]\n",
      "g is [0.19661193 0.1861395  0.24953169 0.19285769 0.20083742]\n",
      "g is [0.19661193 0.19907353 0.24999993 0.19915185 0.21309865]\n",
      "g is [0.19661193 0.22083435 0.24712698 0.20629186 0.23346195]\n",
      "                                  Part Name |     Score | Feedback\n",
      "                                  --------- |     ----- | --------\n",
      "              Feedforward and Cost Function |  30 /  30 | Nice work!\n",
      "                  Regularized Cost Function |  15 /  15 | Nice work!\n",
      "                           Sigmoid Gradient |   5 /   5 | Nice work!\n",
      "  Neural Network Gradient (Backpropagation) |  40 /  40 | Nice work!\n",
      "                       Regularized Gradient |   0 /  10 | \n",
      "                                  --------------------------------\n",
      "                                            |  90 / 100 |  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grader[4] = nnCostFunction\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "### 2.5 Regularized Neural Network\n",
    "\n",
    "After you have successfully implemented the backpropagation algorithm, you will add regularization to the gradient. To account for regularization, it turns out that you can add this as an additional term *after* computing the gradients using backpropagation.\n",
    "\n",
    "Specifically, after you have computed $\\Delta_{ij}^{(l)}$ using backpropagation, you should add regularization using\n",
    "\n",
    "$$ \\begin{align} \n",
    "& \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} & \\qquad \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} + \\frac{\\lambda}{m} \\Theta_{ij}^{(l)} & \\qquad \\text{for } j \\ge 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that you should *not* be regularizing the first column of $\\Theta^{(l)}$ which is used for the bias term. Furthermore, in the parameters $\\Theta_{ij}^{(l)}$, $i$ is indexed starting from 1, and $j$ is indexed starting from 0. Thus, \n",
    "\n",
    "$$\n",
    "\\Theta^{(l)} = \\begin{bmatrix}\n",
    "\\Theta_{1,0}^{(i)} & \\Theta_{1,1}^{(l)} & \\cdots \\\\\n",
    "\\Theta_{2,0}^{(i)} & \\Theta_{2,1}^{(l)} & \\cdots \\\\\n",
    "\\vdots &  ~ & \\ddots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "[Now modify your code that computes grad in `nnCostFunction` to account for regularization.](#nnCostFunction)\n",
    "\n",
    "After you are done, the following cell runs gradient checking on your implementation. If your code is correct, you should expect to see a relative difference that is less than 1e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.27825235e-03 -9.27825236e-03]\n",
      " [-1.67679797e-02 -1.67679797e-02]\n",
      " [-6.01744725e-02 -6.01744725e-02]\n",
      " [-1.73704651e-02 -1.73704651e-02]\n",
      " [ 8.89911959e-03  8.89911960e-03]\n",
      " [ 3.94334829e-02  3.94334829e-02]\n",
      " [-3.19612287e-02 -3.19612287e-02]\n",
      " [-5.75658668e-02 -5.75658668e-02]\n",
      " [-8.36010761e-03 -8.36010762e-03]\n",
      " [ 5.93355565e-02  5.93355565e-02]\n",
      " [ 2.49225535e-02  2.49225535e-02]\n",
      " [-4.51963845e-02 -4.51963845e-02]\n",
      " [ 7.62813550e-03  7.62813551e-03]\n",
      " [ 2.47640974e-02  2.47640974e-02]\n",
      " [ 5.97717617e-02  5.97717617e-02]\n",
      " [ 9.14587966e-03  9.14587966e-03]\n",
      " [-6.74798370e-03 -6.74798370e-03]\n",
      " [-3.26881426e-02 -3.26881426e-02]\n",
      " [ 3.86410548e-02  3.86410548e-02]\n",
      " [ 5.46101547e-02  5.46101547e-02]\n",
      " [ 3.14544970e-01  3.14544970e-01]\n",
      " [ 1.18682669e-01  1.18682669e-01]\n",
      " [ 2.03987128e-01  2.03987128e-01]\n",
      " [ 1.25698067e-01  1.25698067e-01]\n",
      " [ 1.76337550e-01  1.76337550e-01]\n",
      " [ 1.32294136e-01  1.32294136e-01]\n",
      " [ 1.11056588e-01  1.11056588e-01]\n",
      " [ 3.81928666e-05  3.81928696e-05]\n",
      " [ 1.17148233e-01  1.17148233e-01]\n",
      " [-4.07588279e-03 -4.07588279e-03]\n",
      " [ 1.13133142e-01  1.13133142e-01]\n",
      " [-4.52964427e-03 -4.52964427e-03]\n",
      " [ 9.74006970e-02  9.74006970e-02]\n",
      " [ 3.36926556e-02  3.36926556e-02]\n",
      " [ 7.54801264e-02  7.54801264e-02]\n",
      " [ 1.69677090e-02  1.69677090e-02]\n",
      " [ 8.61628953e-02  8.61628953e-02]\n",
      " [ 1.50048382e-03  1.50048382e-03]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 2.25401e-11\n",
      "\n",
      "\n",
      "Cost at (fixed) debugging parameters (w/ lambda = 3.000000): 0.576051 \n",
      "(for lambda = 3, this value should be about 0.576051)\n"
     ]
    }
   ],
   "source": [
    "#  Check gradients by running checkNNGradients\n",
    "lambda_ = 3\n",
    "utils.checkNNGradients(nnCostFunction, lambda_)\n",
    "\n",
    "# Also output the costFunction debugging values\n",
    "debug_J, _  = nnCostFunction(nn_params, input_layer_size,\n",
    "                          hidden_layer_size, num_labels, X, y, lambda_)\n",
    "\n",
    "print('\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' % (lambda_, debug_J))\n",
    "print('(for lambda = 3, this value should be about 0.576051)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submitting Solutions | Programming Exercise neural-network-learning\n",
      "\n",
      "Use token from last successful submission (szong@bcgsc.ca)? (Y/n): y\n",
      "g is [[0.06866401 0.08484199 0.10733201 0.13679999 0.17209629 0.20861413\n",
      "  0.23777833 0.24995594 0.24039993 0.21287124]\n",
      " [0.05758392 0.05047142 0.04649902 0.04517786 0.04634204 0.05013784\n",
      "  0.05703332 0.06783488 0.08365743 0.10572272]\n",
      " [0.23912422 0.21075806 0.17441405 0.13886289 0.10896913 0.08605137\n",
      "  0.06951302 0.05814991 0.05081717 0.04666625]]\n",
      "                                  Part Name |     Score | Feedback\n",
      "                                  --------- |     ----- | --------\n",
      "              Feedforward and Cost Function |  30 /  30 | Nice work!\n",
      "                  Regularized Cost Function |  15 /  15 | Nice work!\n",
      "                           Sigmoid Gradient |   5 /   5 | Nice work!\n",
      "  Neural Network Gradient (Backpropagation) |  40 /  40 | Nice work!\n",
      "                       Regularized Gradient |  10 /  10 | Nice work!\n",
      "                                  --------------------------------\n",
      "                                            | 100 / 100 |  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grader[5] = nnCostFunction\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Learning parameters using `scipy.optimize.minimize`\n",
    "\n",
    "After you have successfully implemented the neural network cost function\n",
    "and gradient computation, the next step we will use `scipy`'s minimization to learn a good set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  After you have completed the assignment, change the maxiter to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxiter': 100}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 1\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params = res.x\n",
    "        \n",
    "# Obtain Theta1 and Theta2 back from nn_params\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training completes, we will proceed to report the training accuracy of your classifier by computing the percentage of examples it got correct. If your implementation is correct, you should see a reported\n",
    "training accuracy of about 95.3% (this may vary by about 1% due to the random initialization). It is possible to get higher training accuracies by training the neural network for more iterations. We encourage you to try\n",
    "training the neural network for more iterations (e.g., set `maxiter` to 400) and also vary the regularization parameter $\\lambda$. With the right learning settings, it is possible to get the neural network to perfectly fit the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 95.760000\n"
     ]
    }
   ],
   "source": [
    "pred = utils.predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred == y) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Visualizing the Hidden Layer\n",
    "\n",
    "One way to understand what your neural network is learning is to visualize what the representations captured by the hidden units. Informally, given a particular hidden unit, one way to visualize what it computes is to find an input $x$ that will cause it to activate (that is, to have an activation value \n",
    "($a_i^{(l)}$) close to 1). For the neural network you trained, notice that the $i^{th}$ row of $\\Theta^{(1)}$ is a 401-dimensional vector that represents the parameter for the $i^{th}$ hidden unit. If we discard the bias term, we get a 400 dimensional vector that represents the weights from each input pixel to the hidden unit.\n",
    "\n",
    "Thus, one way to visualize the “representation” captured by the hidden unit is to reshape this 400 dimensional vector into a 20 × 20 image and display it (It turns out that this is equivalent to finding the input that gives the highest activation for the hidden unit, given a “norm” constraint on the input (i.e., $||x||_2 \\le 1$)). \n",
    "\n",
    "The next cell does this by using the `displayData` function and it will show you an image with 25 units,\n",
    "each corresponding to one hidden unit in the network. In your trained network, you should find that the hidden units corresponds roughly to detectors that look for strokes and other patterns in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.displayData(Theta1[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Optional (ungraded) exercise\n",
    "\n",
    "In this part of the exercise, you will get to try out different learning settings for the neural network to see how the performance of the neural network varies with the regularization parameter $\\lambda$ and number of training steps (the `maxiter` option when using `scipy.optimize.minimize`). Neural networks are very powerful models that can form highly complex decision boundaries. Without regularization, it is possible for a neural network to “overfit” a training set so that it obtains close to 100% accuracy on the training set but does not as well on new examples that it has not seen before. You can set the regularization $\\lambda$ to a smaller value and the `maxiter` parameter to a higher number of iterations to see this for youself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
